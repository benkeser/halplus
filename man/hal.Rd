% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hal.R, R/hal_new.R
\name{hal}
\alias{hal}
\alias{hal}
\title{hal}
\usage{
hal(Y, X, newX = NULL, family = gaussian(), verbose = FALSE,
  obsWeights = rep(1, length(Y)), sparseMat = TRUE,
  nfolds = ifelse(length(Y) <= 100, 20, 10), nlambda = 100,
  minVars = NULL, maxDim = 10, useMin = TRUE, debug = TRUE,
  parallel = FALSE, ...)

hal(Y, X, newX = NULL, family = gaussian(), verbose = FALSE,
  obsWeights = rep(1, length(Y)), sparseMat = TRUE,
  nfolds = ifelse(length(Y) <= 100, 20, 10), nlambda = 100,
  minVars = NULL, maxDim = 10, useMin = TRUE, debug = TRUE,
  parallel = FALSE, ...)
}
\arguments{
\item{Y}{A \code{numeric} of outcomes}

\item{X}{A \code{data.frame} of predictors}

\item{newX}{Optional \code{data.frame} on which to return predicted values}

\item{family}{Statistical family: Gaussian and Binomial have been tested.}

\item{verbose}{A \code{boolean} indicating whether to print output on functions progress}

\item{obsWeights}{Optional \code{vector} of observation weights to be passed to \code{cv.glmnet}}

\item{sparseMat}{Use an implementation based on sparse matrices or normal
(i.e., non-sparse) matrices. The normal matrix implementation is old and
not guaranteed to work correctly (will likely be deprecated soon).}

\item{nfolds}{Number of CV folds passed to \code{cv.glmnet}}

\item{nlambda}{Number of lambda values to search across in \code{cv.glmnet}}

\item{minVars}{The minimum number of variables for which coefficients are to
be estimated; this is used to perform variable filtering prior to fitting
the LASSO model via cross-validation.}

\item{maxDim}{The maximum tolerable dimension of the matrix of variables X.
If the dimension of the supplied matrix X exceeds this value, then a small
number of relevant variables are automatically selected for use with the
cross-validated LASSO estimator. This overrides \code{minVars}.}

\item{useMin}{Option passed to \code{cv.glmnet}, use minimum risk lambda or 1se lambda (more
penalization)}

\item{debug}{For benchmarking. Setting to \code{TRUE} will run garbage collection to
improve the accuracy of memory monitoring}

\item{parallel}{A boolean indicating whether to use a parallel backend, if possible}

\item{...}{Not currently used}

\item{Y}{outcome}

\item{X}{data}

\item{newX}{New data to apply the model fit and generate predictions.}

\item{verbose}{Set to \code{TRUE} for more detailed output.}

\item{obsWeights}{Weights to be given to observations.}

\item{nfolds}{Number of cross-validation folds, passed directly to
\code{glmnet::cv.glmnet}.}

\item{nlambda}{Number of lambda values to test in fitting the LASSO model,
passed directly to \code{glmnet::cv.glmnet}.}

\item{useMin}{option passed directly to \code{glmnet::cv.glmnet}: whether to
use the value of lambda that minimimzes the mean cross-validated error or
the largest value of lambda such that error is within 1 standard error of
the minimum (consult help for \code{glmnet::cv.glmnet} for more info).}

\item{debug}{Set to \code{TRUE} to run garbage collection (\code{gc}), to
improve the accuracy of memory monitoring.}

\item{parallel}{Use a registered parallel backend if possible.}

\item{...}{Any extra arguments (unused).}
}
\description{
The highly adaptive lasso fitting function. This function takes a matrix of predictor values
(which can be binary or continuous) and converts it into a set of indicator basis functions
that perfectly fit the data. The function then uses cross-validated lasso (via the \code{glmnet}
package) to select basis functions. The resulting fit is called the highly adaptive lasso.
The process of creating the indicator basis functions can be extremely time and memory intensive
as it involves creating n(2^d - 1) basis functions, where n is the number of observations
and d the number of covariates. The function also must subsequently search over basis functions
for those that are duplicated and store the results. Future implementations will attempt to
streamline this process to the largest extent possible; however, for the time being implementing
with values of n and d such that n(2^d - 1) > 1e7 is not recommended.

Model fitting function for the highly adaptive LASSO estimator.
}
